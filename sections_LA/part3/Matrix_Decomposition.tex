Matrix decomposition is very very crucial for Machine Learning. In real life, a lot of matrices that are created as a final product are often product of two or more matrices that represent different things. Recovering the individual matrices from the resultant matrix is always an important thing to do. 


\section{SVD Decomposition}

The Singular Value Decomposition (SVD) is a factorization or decomposition of a matrix into three other matrices, which, when multiplied together, reconstruct the original matrix. For an $m \times n$ matrix $A$, the SVD is given by:

\[ A = U \Sigma V^T \]

where:
\begin{itemize}
    \item $U$ is an $m \times m$ orthogonal matrix (its columns are the left singular vectors of $A$),
    \item $\Sigma$ is an $m \times n$ diagonal matrix with non-negative real numbers on the diagonal (the singular values of $A$),
    \item $V^T$ is the transpose of an $n \times n$ orthogonal matrix $V$ (its columns are the right singular vectors of $A$).
\end{itemize}

The singular values on the diagonal of $\Sigma$ are ordered in descending order. The singular vectors in $U$ and $V$ are orthonormal, meaning that $U^T U = I$ and $V^T V = I$, where $I$ is the identity matrix.

SVD has numerous applications in various fields, including linear algebra, statistics, signal processing, and machine learning. It is a powerful tool for understanding the structure of a matrix, extracting important features, and solving linear systems. Additionally, it plays a crucial role in techniques like Principal Component Analysis (PCA) and is widely used in data compression and dimensionality reduction.

\subsection{Procedure}

The Singular Value Decomposition (SVD) algorithm involves several steps to decompose a given matrix \( A \) into the product of three matrices \( U, \Sigma, \) and \( V^T \). Here are the detailed steps of the SVD algorithm:

\textbf{Input:} An \( m \times n \) matrix \( A \).

\textbf{Output:} Matrices \( U, \Sigma, \) and \( V^T \) such that \( A = U \Sigma V^T \).

\begin{enumerate}
    \item \textbf{Compute the Gram matrix:}
        \begin{itemize}
            \item Compute \( A^T A \) (or \( A A^T \), depending on whether \( m \) or \( n \) is larger).
            \item Find the eigenvalues and eigenvectors of \( A^T A \) (or \( A A^T \)).
        \end{itemize}

    \item \textbf{Compute the singular values:}
        \begin{itemize}
            \item The singular values are the square roots of the eigenvalues obtained in step 1.
            \item Order the singular values in descending order: \( \sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_p \), where \( p = \min(m, n) \).
        \end{itemize}

    \item \textbf{Compute the singular vectors:}
        \begin{itemize}
            \item For each singular value \( \sigma_i \), find the corresponding singular vector by solving \( A^T A \mathbf{v}_i = \sigma_i^2 \mathbf{v}_i \) (or \( A A^T \mathbf{u}_i = \sigma_i^2 \mathbf{u}_i \)).
        \end{itemize}

    \item \textbf{Form \( U, \Sigma, \) and \( V^T \):}
        \begin{itemize}
            \item Form the matrices \( U \) and \( V \) by using the computed singular vectors as columns.
            \item Form the diagonal matrix \( \Sigma \) by placing the singular values on the diagonal.
        \end{itemize}
        \[ U = [\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_m], \]
        \[ V = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n], \]
        \[ \Sigma = \text{diag}(\sigma_1, \sigma_2, \ldots, \sigma_p). \]

    \item \textbf{Check for rectangular matrices:}
        \begin{itemize}
            \item If \( A \) is rectangular (i.e., \( m \neq n \)), append additional columns to \( U \) or \( V \) accordingly.
        \end{itemize}
\end{enumerate}

The resulting matrices \( U, \Sigma, \) and \( V^T \) satisfy the SVD equation \( A = U \Sigma V^T \).



\subsection{Algorithm}

\begin{algorithm}
\caption{Singular Value Decomposition (SVD)}
\begin{algorithmic}[1]
    \Require Matrix $A$ of size $m \times n$
    \Ensure Matrices $U$, $\Sigma$, and $V^T$ such that $A = U \Sigma V^T$
    
    \State \textbf{Compute Gram matrix:}
    \If {$m \geq n$}
        \State $C \gets A^T A$
    \Else
        \State $C \gets A A^T$
    \EndIf
    
    \State Compute eigenvalues and eigenvectors of $C$ \Comment{This can be done using numerical methods}
    
    \State \textbf{Compute singular values:}
    \State Extract singular values $\sigma_i$ from the eigenvalues of $C$
    \State Order singular values in descending order: $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_p$, where $p = \min(m, n)$
    
    \State \textbf{Compute singular vectors:}
    \For {$i = 1$ to $p$}
        \State Solve $A^TA \mathbf{v}_i = \sigma_i^2 \mathbf{v}_i$ for right singular vectors
        \State Solve $AA^T \mathbf{u}_i = \sigma_i^2 \mathbf{u}_i$ for left singular vectors
    \EndFor
    
    \State \textbf{Form $U, \Sigma, $ and $V^T$:}
    \State Form matrix $U$ by using left singular vectors as columns
    \State Form matrix $V$ by using right singular vectors as columns
    \State Form diagonal matrix $\Sigma$ with singular values on the diagonal
    
    \If {$m \neq n$}
        \State Append additional columns to $U$ or $V$ accordingly \Comment{For rectangular matrices}
    \EndIf
    
    \State \textbf{Output:} Matrices $U, \Sigma, $ and $V^T$
\end{algorithmic}
\end{algorithm}


\textbf{Now, clearly this algorithm needs to solve an Eigen value problem that we will discuss later. Generalized Eigen value problems are a significant part of numerical linear algebra that it is imperative to cover that in great detail.}



\section{QR Decomposition}

For an $m \times n$ matrix $A$, the QR decomposition is given by:
\[ A = QR \]

where:
\begin{itemize}
    \item $Q$ is an $m \times m$ orthogonal matrix (its columns are orthonormal vectors),
    \item $R$ is an $m \times n$ upper triangular matrix.
\end{itemize}



\subsection{Procedure}
There are several procedures to produce the QR factorization of a matrix. Gram Schmidt procedure(Classical and Modified) are used to achieve the same widely. Intuitively speaking it is evident that the upper triangular matrix R is a representation of the old basis vectors with respect to the new basis vectors and the new basis vectors constitute the matrix Q.

Given a matrix $A$ with linearly independent columns:

\begin{enumerate}
    \item \textbf{Initialize:} Let $A = [a_1, a_2, \ldots, a_n]$ be the matrix with linearly independent columns.
    
    \item \textbf{Initialize $Q$:} Set $Q = []$, an empty matrix to be filled with orthonormal vectors.
    
    \item \textbf{Iterate for each column:}
    \begin{itemize}
        \item For $i = 1$ to $n$:
        \begin{itemize}
            \item $v_i = a_i$ (initialize $v_i$ as the current column vector).
            \item \textbf{Orthogonalization Step:}
            \begin{itemize}
                \item For $j = 1$ to $i-1$:
                \begin{itemize}
                    \item $v_i = v_i - \text{proj}_{u_j}(a_i)$, where $\text{proj}_{u_j}(a_i) = \frac{\langle a_i, u_j \rangle}{\langle u_j, u_j \rangle}u_j$.
                \end{itemize}
            \end{itemize}
            \item \textbf{Normalization Step:}
            \begin{itemize}
                \item $u_i = \frac{v_i}{\|v_i\|}$ (normalize $v_i$ to obtain $u_i$).
            \end{itemize}
            \item Append $u_i$ as a column to $Q$.
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Form $R$:} The $R$ matrix is an upper triangular matrix formed by the dot products of the original vectors with the orthonormal vectors:
    \begin{itemize}
        \item For $i = 1$ to $n$:
        \begin{itemize}
            \item For $j = 1$ to $i$:
            \begin{itemize}
                \item $r_{ij} = \langle a_j, u_i \rangle$.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{enumerate}

\textbf{Result:} The matrices $Q$ and $R$ obtained in the process satisfy $A = QR$.

\subsection{A more intutive look at why Gram Schmidt procedure provides us with a QR factorization.}

\textbf{Each step of MGS procedure is like multiplying an upper trianglular matrix to the original matrix A, whose columns represent the linearly independent vectors that we are starting with.}


In the modified GS approach we subtract from each next vector, a component from the first vector and we keep on doing the same.

This is equivalent to right-multiplication by a matrix $R_1$ :
$$
\left[\begin{array}{l|l|l|l} 
& & & \\
v_1 & v_2 & \ldots & v_n \\
& & &
\end{array}\right]\left[\begin{array}{cccc}
\frac{1}{r_{11}} & \frac{-r_{12}}{r_{11}} & \cdots & \frac{-r_{1 n}}{r_{11}} \\
& 1 & & \vdots \\
& & \ddots & \vdots \\
& & & 1
\end{array}\right]=\left[q_1\left|v_2^{(2)}\right| \ldots \mid v_n^{(2)}\right]
$$

In general, step $i$ subtracts $r_{i j} / r_{i i}$ times column $i$ of the current $A$ from columns, $j>i$ and replaces column $i$ by $1 / r_{i i}$ times itself. This corresponds to multiplication by upper triangular matrix $R_i$ :
$$
R_2=\left[\begin{array}{cccc}
1 & & & \\
& \frac{1}{r_{22}} & \frac{-r_{23}}{r_{22}} & \cdots \\
& & 1 & \\
& & & \ddots
\end{array}\right] R_3=\left[\begin{array}{cccc}
1 & & & \\
& 1 & & \\
& & \frac{1}{r_{33}} & \cdots \\
& & & \ddots
\end{array}\right], \ldots
$$


Thus we keep on multiplying the original matrix A with upper triangular matrices until it becomes an orthogonal matrix. 



$$A \underbrace{R_1 R_2 \cdots R_n}_{R^{-1}}=Q$$


But there is another way of doing this QR factorization. That is successive multiplication of Q matrices in the left of A until the resultant is an upper triangular matrix. This is called the Householder Algorithm.

\section{Householder Algorithm}

GS procedure is known as Trianguar Orthogonalization. Now we introduce the Householder Algorithm that is \textbf{Orthogonal Triangularization}

We multiply a matrix repeatedly with unitary matrices, until it becomes a triangular matrix. It is basically introducting zeros in the columns of the original matrix in a step by step manner. 

We achieve this by introducing reflectors that reflect the vectors that represent the columns of the matrix across a hyperplane such that it becomes a unit vector. Then we take the second column onwards and apply the same thing repeatedly. 

\subsection*{Householder Transformation Algorithm}

Given a matrix $A$, perform Householder transformations to reduce it to a desired form.

\begin{enumerate}
    \item \textbf{Initialize:} Start with the original matrix $A$.
    
    \item \textbf{For each column or row (k) below the diagonal:}
    \begin{enumerate}
        \item \textbf{Choose vector \(v\):} Let \(x\) be the column vector of the selected column or row starting from element \(k+1\). Choose \(v\) such that \(v = x - \text{sign}(x_k) \cdot \|x\| \cdot e_1\), where \(e_1\) is the standard basis vector.
        
        \item \textbf{Normalize \(v\):} Compute \(\beta = \frac{1}{2}\|v\|^2\). The Householder vector is \(v\) and the Householder matrix is \(H = I - \frac{2}{\beta}vv^T\).
        
        \item \textbf{Apply Householder transformation:} Update \(A\) as \(A := H \cdot A \cdot H^T\). This step zeros out the elements below the diagonal in the chosen column or row.
    \end{enumerate}
    
    \item \textbf{Repeat:} Repeat step 2 for each column or row below the diagonal until the matrix \(A\) reaches the desired form.
\end{enumerate}

\subsection*{Geometrical Interpretation of Householder Algorithm}

The Householder algorithm has a compelling geometrical interpretation. At its core, it performs a series of orthogonal reflections through hyperplanes to progressively eliminate elements below the main diagonal of the matrix. Consider a chosen vector \(v\) for a specific Householder transformation. Geometrically, this vector \(v\) can be visualized as pointing in the direction that reflects the components below the main diagonal. The Householder matrix \(H\) is constructed to perform a reflection across the hyperplane orthogonal to \(v\). This reflection transforms the original vector or subspace, effectively projecting it onto its orthogonal complement and eliminating certain entries in the matrix. Iteratively applying these reflections reorients the matrix until it reaches the desired form. Thus, the Householder algorithm's geometrical intuition lies in systematically reflecting and projecting vectors or subspaces to simplify the structure of a matrix.


\subsection{Algorithms}

\begin{algorithmic}[1]
    \Procedure{HouseholderTransformation}{$A$}
        \State $m, n \gets$ dimensions of $A$
        
        \For{$k \gets 1$ \textbf{to} $\min(m-1, n)$}
            \State $x \gets$ column vector of $A$ starting from element $k+1$
            \State $v \gets x - \text{sign}(x_k) \cdot \text{norm}(x) \cdot e_1$ \Comment{$e_1$ is the standard basis vector}
            
            \State $\beta \gets 0.5 \cdot \text{norm}(v)^2$
            \State Householder vector $v$ and matrix $H \gets I - \frac{2}{\beta}vv^T$
            
            \State $A \gets H \cdot A \cdot H^T$ \Comment{Apply Householder transformation to zero out elements below the diagonal}
        \EndFor
        \State \textbf{Output:} Matrix $A$ transformed by Householder transformations
    \EndProcedure
\end{algorithmic}



