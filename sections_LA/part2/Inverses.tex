\section{Inverses}

The main motive of all this linear algebra business is that we want to solve linear equations that are given by the matrix equations.

$Ax=b \implies x = A^{-1}b$

Clearly, solving this problem is solving an inverse problem. So, we have to know about inverses.

\subsection{Definition of Left and Right Inverse}

\begin{ouline}
Given two sets V, W and a onto function $T: V \to W$ A left inverse $L : W \to V$ is one that satisfies $L(T(x)) = x$, i.e.,
$LT = I_V$ . A right inverse $R : W \to V $satisfies $T(R(x)) = x$,
i.e., $TR = I_W$.
\end{ouline}

\subsection{Properties of Left and Right Inverses}

\begin{enumerate}
    \item Every onto function $f : V \to W$ has at least one right inverse.
Indeed, for every $w \in W$ there is some $v \in V$ so that
$f (v) = w$. Define $R(w) = v$. Clearly $f (R(w)) = f (v) = w$.
In general, right inverses are not unique.

\item 
Here is an interesting result about left inverses: An onto
function $T : V \to W$ can have at most one left inverse. If L is
a left inverse of T then it is also a right inverse!

\textbf{Proof:} Suppose $L_1, L_2 : W \to V$ are left inverses. If
$w = T(v)$, then $L_1(w) = v = L_2(w)$. Hence $L_1 = L_2$ (the
onto assumption plays a role).
$T(L(w)) = T(L(T(v))) = T(v) = w$. Hence, L is also a right
inverse.
\item 
Moreover, if a left inverse exists, the right inverse is THE left
inverse, i.e., the right inverse is unique. (Indeed,
$TR_1 = TR_2 = I$ and hence, $ LTR_1 = LTR_2 \implies R_1 = R_2$.
\end{enumerate}


The existence of a \textbf{ linear inverse} is the signature property of an \textbf{onto the linear map.} Moreover, if T is such a map, $T(x)=0 \iff x=0$

In the next part, we will see the computation of inverses using Gauss-Jordan elimination, where they leverage all the learned concepts into real-life actions. 

From now on, we assume the existence of a left inverse implies the uniqueness of the right inverse, too, implying the existence of a single inverse. 

\subsection{Properties of inverses ass properties of matrices}

Every Linear transformation has associated with it a matrix such that $T(v)=Av$, the inverse of this linear transformation implies the existence of the inverse to this matrix as well. The column space of the matrix A represents the range of T.

\textbf{For a nxn matrix A, the inverse B is defined as AB=BA=I}

The inverse of a matrix only exists if the column rank of that matrix is full. In alternate descriptions, the equation Ax=0 only has trivial solutions. 

\subsection{How to compute Inverses?}

For now, we only learn the concept behind calculating inverses; later on, we learn the actual method. 

If AB=I, then what it means is that:

$$\sum_k A_{ik}B_{kj}=\delta_{ij}$$

Every column of B is an unknown vector where $AB_i=e_i$, where $e_i$ is the basis for the vector on which the matrix A was originally applied. 

It is like solving linear equations, where for each basis $e_i$, we make an augmented matrix, $[A|e_i]$, and solve these simultaneously. If A is invertible, then all these results should come together without any inconsistency and in effect what we will be doing is transforming the augmented matrix $[A|I]$ to $[I| A^{-1}]$

\subsubsection{Examples}

Finding the inverse of the matrix 
$\begin{pmatrix}
    2 & 3 & 4\\
    3 & 8 & 9\\
    1 & 3 & 7\\
\end{pmatrix}$

And the augmented matrix $[A | I]$:
\[
\left(\begin{array}{ccc|ccc}
    2 & 3 & 4 & 1 & 0 & 0\\
    3 & 8 & 9 & 0 & 1 & 0\\
    1 & 3 & 7 & 0 & 0 & 1\\
\end{array}\right)
\]

Now, let's perform row operations to get the identity matrix on the left side:

1. $R_1 \leftrightarrow R_3$ (swap rows 1 and 3):
\[
\left(\begin{array}{ccc|ccc}
    1 & 3 & 7 & 0 & 0 & 1\\
    3 & 8 & 9 & 0 & 1 & 0\\
    2 & 3 & 4 & 1 & 0 & 0\\
\end{array}\right)
\]

2. $R_2 - 3R_1 \rightarrow R_2$ (subtract 3 times the first row from the second row):
\[
\left(\begin{array}{ccc|ccc}
    1 & 3 & 7 & 0 & 0 & 1\\
    0 & -1 & -12 & 0 & 1 & -3\\
    2 & 3 & 4 & 1 & 0 & 0\\
\end{array}\right)
\]

3. $R_3 - 2R_1 \rightarrow R_3$ (subtract 2 times the first row from the third row):
\[
\left(\begin{array}{ccc|ccc}
    1 & 3 & 7 & 0 & 0 & 1\\
    0 & -1 & -12 & 0 & 1 & -3\\
    0 & -3 & -10 & 1 & 0 & -2\\
\end{array}\right)
\]

4. $-R_2 \rightarrow R_2$ (multiply the second row by -1):
\[
\left(\begin{array}{ccc|ccc}
    1 & 3 & 7 & 0 & 0 & 1\\
    0 & 1 & 12 & 0 & -1 & 3\\
    0 & -3 & -10 & 1 & 0 & -2\\
\end{array}\right)
\]

5. $R_1 - 3R_2 \rightarrow R_1$ (subtract 3 times the second row from the first row):
\[
\left(\begin{array}{ccc|ccc}
    1 & 0 & -29 & 0 & 3 & -8\\
    0 & 1 & 12 & 0 & -1 & 3\\
    0 & -3 & -10 & 1 & 0 & -2\\
\end{array}\right)
\]

6. $R_3 + 3R_2 \rightarrow R_3$ (add 3 times the second row to the third row):
\[
\left(\begin{array}{ccc|ccc}
    1 & 0 & -29 & 0 & 3 & -8\\
    0 & 1 & 12 & 0 & -1 & 3\\
    0 & 0 & 26 & 1 & -3 & 7\\
\end{array}\right)
\]

7. $R_3/26 \rightarrow R_3$ (divide the third row by 26):
\[
\left(\begin{array}{ccc|ccc}
    1 & 0 & -29 & 0 & 3 & -8\\
    0 & 1 & 12 & 0 & -1 & 3\\
    0 & 0 & 1 & \frac{1}{26} & -\frac{3}{26} & \frac{7}{26}\\
\end{array}\right)
\]

8. $R_1 + 29R_3 \rightarrow R_1$ (add 29 times the third row to the first row):
\[
\left(\begin{array}{ccc|ccc}
    1 & 0 & 0 & \frac{29}{26} & -\frac{84}{26} & \frac{191}{26}\\
    0 & 1 & 12 & 0 & -1 & 3\\
    0 & 0 & 1 & \frac{1}{26} & -\frac{3}{26} & \frac{7}{26}\\
\end{array}\right)
\]

9. $R_2 - 12R_3 \rightarrow R_2$ (subtract 12 times the third row from the second row):
\[
\left(\begin{array}{ccc|ccc}
    1 & 0 & 0 & \frac{29}{26} & -\frac{84}{26} & \frac{191}{26}\\
    0 & 1 & 0 & -\frac{12}{26} & \frac{35}{26} & -\frac{33}{26}\\
    0 & 0 & 1 & \frac{1}{26} & -\frac{3}{26} & \frac{7}{26}\\
\end{array}\right)
\]

So, the inverse of the given matrix is:
\[
\boxed{
    A^{-1} = \begin{pmatrix}
        \frac{29}{26} & -\frac{84}{26} & \frac{191}{26}\\
        -\frac{12}{26} & \frac{35}{26} & -\frac{33}{26}\\
        \frac{1}{26} & -\frac{3}{26} & \frac{7}{26}\\
    \end{pmatrix}
}
\]








